{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from opt_utils import load_params_and_grads, forward_propagation, backward_propagation\n",
    "from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "\n",
    "\n",
    "from scipy import ndimage\n",
    "from dnn_utils import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path, dirs, cat_files_train = next(os.walk(\"training_set/cats/\"))\n",
    "file_count_cat = len(cat_files_train)\n",
    "print(file_count_cat)\n",
    "\n",
    "path, dirs, dog_files_train = next(os.walk(\"training_set/dogs/\"))\n",
    "file_count_dog = len(dog_files_train)\n",
    "print(file_count_dog)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = 200\n",
    "n_dog = 200\n",
    "\n",
    "#Variables\n",
    "m = n_cat + n_dog               # Number of train ex.\n",
    "print(m)\n",
    "num_px = 64\n",
    "label_y = np.zeros((1,m))\n",
    "X_train = np.zeros((12288,m))\n",
    "# Input Dataset from the folder\n",
    "\n",
    "for i in range(m):\n",
    "#     print(i%n_cat)\n",
    "    if i<n_cat:\n",
    "        my_image = \"cat.\"+str(i+1)+\".jpg\" # change this to the name of your image file \n",
    "        fname = \"training_set/cats/\" + my_image\n",
    "        image = np.array(ndimage.imread(fname, flatten=False))\n",
    "        my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "        label_y[0][i] = 1 # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "        X_train[:,i] = my_image[:,0]\n",
    "        \n",
    "    else:\n",
    "        my_image = \"dog.\"+str((i+1-n_cat))+\".jpg\" # change this to the name of your image file \n",
    "        fname = \"training_set/dogs/\" + my_image\n",
    "        image = np.array(ndimage.imread(fname, flatten=False))\n",
    "        my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "        label_y[0][i] = 0\n",
    "        X_train[:,i] = my_image[:,0]\n",
    "    \n",
    "\n",
    "#     fname = \"test_set/cats/\" + my_image\n",
    "#     image = np.array(ndimage.imread(fname, flatten=False))\n",
    "#     my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "    # my_predicted_image = %precisiondict(my_image, my_label_y, parameters)\n",
    "# print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + \n",
    "#        classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n",
    "\n",
    "# print(train_y[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "print(X_train.shape)\n",
    "print(label_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train/255\n",
    "# train_x = train_x.T\n",
    "train_y = label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 12288     # num_px * num_px * 3\n",
    "layers_dims = [12288,10,10,10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "#     np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,mini_batch_size*k:mini_batch_size*(k+1)]\n",
    "        mini_batch_Y = shuffled_Y[:,mini_batch_size*k:mini_batch_size*(k+1)]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,mini_batch_size*(num_complete_minibatches):m]\n",
    "        mini_batch_Y = shuffled_Y[:,mini_batch_size*(num_complete_minibatches):m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "    ### START CODE HERE ### (approx. 4 lines)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)] + (1-beta1)*grads[\"dW\"+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)] + (1-beta1)*grads[\"db\"+str(l+1)]\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**t)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)] + (1-beta2)*(grads[\"dW\"+str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)] + (1-beta2)*(grads[\"db\"+str(l+1)]**2)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**t)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*v_corrected[\"dW\"+str(l+1)]/np.sqrt(s_corrected[\"dW\"+str(l+1)]+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v_corrected[\"db\"+str(l+1)]/np.sqrt(s_corrected[\"db\"+str(l+1)]+epsilon)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "#     seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "#         seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = L_layer_model(train_x,train_y , layers_dims, num_iterations = 2000, print_cost = True)\n",
    "\n",
    "# train 3-layer model\n",
    "layers_dims = [train_x.shape[0], 5, 2, 1]\n",
    "parameters = model(train_x, train_y, layers_dims, optimizer = \"gd\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_x, train_y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "# plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, dirs, cat_files_test = next(os.walk(\"test_set/cats/\"))\n",
    "file_count_cat = len(cat_files_test)\n",
    "print(file_count_cat)\n",
    "\n",
    "path, dirs, dog_files_test = next(os.walk(\"test_set/dogs/\"))\n",
    "file_count_dog = len(dog_files_test)\n",
    "print(file_count_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat_test = 50\n",
    "n_dog_test = 50\n",
    "\n",
    "#Variables\n",
    "m_test = n_cat_test + n_dog_test               # Number of train ex.\n",
    "print(m_test)\n",
    "num_px = 64\n",
    "y_test = np.zeros((1,m_test))\n",
    "X_test = np.zeros((12288,m_test))\n",
    "# Input Dataset from the folder\n",
    "\n",
    "for i in range(4000,m_test+4000):\n",
    "#     print(i%n_cat)\n",
    "    if i<n_cat_test+4000:\n",
    "        my_image = \"cat.\"+str(i+1)+\".jpg\" # change this to the name of your image file \n",
    "        fname = \"test_set/cats/\" + my_image\n",
    "        image = np.array(ndimage.imread(fname, flatten=False))\n",
    "        my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "        y_test[0][i%4000] = 1 # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "#         print(my_image)\n",
    "        X_test[:,i%4000] = my_image[:,0]\n",
    "#         print(X_train)\n",
    "        \n",
    "    else:\n",
    "        my_image = \"dog.\"+str(i+1-n_cat_test)+\".jpg\" # change this to the name of your image file \n",
    "        fname = \"test_set/dogs/\" + my_image\n",
    "        image = np.array(ndimage.imread(fname, flatten=False))\n",
    "        my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "        y_test[0][i%4000] = 0\n",
    "        X_test[:,i%4000] = my_image[:,0]\n",
    "    \n",
    "\n",
    "#     fname = \"test_set/cats/\" + my_image\n",
    "#     image = np.array(ndimage.imread(fname, flatten=False))\n",
    "#     my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "    # my_predicted_image = %precisiondict(my_image, my_label_y, parameters)\n",
    "# print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + \n",
    "#        classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n",
    "\n",
    "# print(train_y[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = X_test\n",
    "test_y = y_test \n",
    "pred_test = predict(test_x, test_y, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
